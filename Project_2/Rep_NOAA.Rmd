---
title: "Finding the most severe storms in the US"
author: "Juan C. López Tavera"
date: "9/22/2017"
output: github_document
---

## Introduction

This is a reproducible research report made with knitr for completing the requirements of the Reproducible Research Course by Johns Hopkins University at Coursera. 

This documents describe the exploration of U.S. National Oceanic and Atmospheric Administration's (NOAA) storm database, which tracks weather phenomena, their characteristics and casualties caused by them. 

The main objective of this data exploration is to answer the questions:

1. Across the USA, which types of events are most harmful with respect to population health?
2. Across the USA, which types of events have the greatest economic consequences?

#### Project Setup

```{r project setup, include=TRUE, message=FALSE, warning=FALSE}
## Loading the required packages for reproducing the report
pkgs <-
  c("data.table",
    "R.utils",
    "tidyverse",
    "knitr",
    "datasets",
    "stringdist")

f <- unlist(lapply(pkgs, require, character.only = TRUE))
if (length(f[!f] > 0)) {
  install.packages(pkgs[!f])
}

invisible(sapply(X = pkgs, FUN = library, character.only = TRUE))


## Setting up all code chunks according to the assignment specifications
knitr::opts_chunk$set(
    eval = TRUE,
    echo = TRUE,
    tidy = TRUE,
    results = "markup",
    include = TRUE,
    message = FALSE,
    warning = FALSE,
    knitr.table.format = "markdown",
    tidy.opts = list(width.cutoff = 80),
    fig.align = "center",
    fig.path = "figure/",
    highlight = TRUE, 
    cache = TRUE
)

```


#### Session Info

```{r sesion info}

sessionInfo()

```


## Data reading

First, if necessary, we download the data BZ2 file from the link provided in the course assignment page; then, we read the data into the working environment, and conveniently cache the result of the computation.  

```{r data loading, cache=TRUE, message=FALSE}
## Downloading the data from the URL in the course assignment page
URL <-
  "https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2"
path <- paste0(getwd(), "/repdata%2Fdata%2FStormData.csv.bz2")
csvfile <- gsub(pattern = ".bz2",
                replacement = "",
                x = path)

## If the file does not exist, we download it, 
## If it exist and it hasn't been uncompressed, we uncompress it
## Then we read it into the R environment.
if (!file.exists(path)) {
  download.file(url = URL,
                destfile = path)
  R.utils::bunzip2(path, remove = FALSE)
  storm_data <- data.table::fread(input = csvfile,
                                showProgress = FALSE,
                                verbose = FALSE, 
                                na.strings = c("", " ", "NA"))
  
} else if (!file.exists(csvfile)) {
  R.utils::bunzip2(path, remove = FALSE)
  storm_data <- data.table::fread(input = csvfile,
                                showProgress = FALSE,
                                verbose = FALSE, 
                                na.strings = c("", " ", "NA"))
  
} else {
  storm_data <- data.table::fread(input = csvfile,
                                showProgress = FALSE,
                                verbose = FALSE, 
                                na.strings = c("", " ", "NA"))
}

```

## Dataset Structure

We have a tibble data frame of `r nrow(storm_data)` rows by `r ncol(storm_data)` columns, with `r sum(is.na(storm_data))` missing values, which are `r round(mean(is.na(storm_data))*100, 2)`% of the observations.

### Variables of interest

The NOAA storm data set we're using has many variables, but not all of them fall into the scope of this report, which focuses on knowing the human and economic damages caused by reported weather events. Knowing beforehand which variables we're using will speed-up the data processing and analysis.

From the `r ncol(storm_data)` variables in the data set, we're only focusing on: 

1. `REFNUM`: The reference number assigned to each weather event.
2. `STATE`: The state name abbreviation where each weather event occurred. 
3. `COUNTYNAME`: The county where each weather event occurred. 
4. `BGN_DATE`: The reported date when each weather event was was first noticeable.
5. `END_DATE`: The reported date when each weather event ceased. 
6. `EVTYPE`: The classification of each weather event as one of the 48 types defined by the NOAA.
7. `FATALITIES`: The number of direct and indirect human lives lost due to each weather event. 
8. `INJURIES`: The number of reported injuries caused by each weather event.
9. `PROPDMG`: The cost of property damage in USD (raw)
10. `PROPDMGEXP`: The powers of 10 by which `PROPDMG` shall be multiplied to arrive at the final property damage cost in USD.
11. `CROPDMG`: The cost of agricultural losses/damage in USD (raw)
12. `CROPDMGEXP`: The powers of 10 by which `CROPDMG` shall be multiplied to arrive at the final property damage cost in USD.

Now, we subset the NOAA data set: 

```{r subsetting the NOAA dataset}

storm_data <- storm_data %>%
  select(
  REFNUM,
  STATE,
  COUNTYNAME,
  BGN_DATE,
  END_DATE,
  EVTYPE,
  FATALITIES,
  INJURIES,
  PROPDMG,
  PROPDMGEXP,
  CROPDMG,
  CROPDMGEXP
  )

```

So we have a tibble data frame —subset of the NOAA data set provided at the Reproducible Research Course— of `r nrow(storm_data)` rows by `r ncol(storm_data)` columns, with `r sum(is.na(storm_data))` missing values, which are `r round(mean(is.na(storm_data))*100, 2)`% of the observations.

The classes of our NOAA storm database, `storm_data`, and the proportion of missing values for each column are shown in the following table:

```{r data processing}
## We make a tbl of column classes, the percentage of NAs; and display it
classes <-
  sapply(storm_data, class) %>%
  as_data_frame() %>%
  rownames_to_column("Variable") %>%
  select(Variable, Type = value)
## Percentage of missing values, rounded to two decimals
classes$`Percent of NAs` <- paste(sapply(storm_data, function(x)
        round(mean(is.na(x)) * 100, 2)), "%")

## Number of unique values
classes$`Unique Values` <- sapply(storm_data, function(x)length(unique(x)))

## Print table with nice formatting
kable(x = classes, align = c("l", "c", "r", "r"))

```

Looking at the table, we can notice a couple of major oddities: 

1. The number of unique States in `STATE` is 72, while it should be 50.
2. The number of different events `EVTYPE` is 985, while the NOAA has defined just 48.
3. It is already known that `PROPDMG` and `CROPDMG` are not expressed as final USD amounts, but need to be multiplied by the factors `PROPDMG` and `PROPDMG` respectively. `PROPDMG` and `PROPDMG` should be of equal length though. 

What causes such incongruities? How can we make our dataset more consistent with the phenomena it characterizes? These are questions that we shall answer in the Data Procesing section. 

## Data Processing 

#### Solving the state abbreviations issue

As mentioned above, we need to investigate why to we have 72 states instead of 50. Conveniently, the `datasets` package includes the full and abbreviated names of US states, which we can use to match with our dataset abbreviations. Let's take a closer look at the non-matching state abbreviations:

```{r state names and abbreviations}

data(state)
states <- cbind.data.frame(state.abb, state.name)

setdiff(x = storm_data$STATE, y = states$state.abb)

```

We can tell that some of those abbreviations are US District, Territories, water bodies and regions. We manually record those values and check if there are still any unmatches. 

```{r}

places <-
  c(
  DC = "District of Columbia",
  PR = "Puerto Rico",
  AS = "American Samoa",
  GU = "Guam",
  MH = "Marshall Islands",
  VI = "Virgin Islands",
  LO = "Lake Ontario",
  LE = "Lake Erie",
  LS = "Lake Superior",
  LM = "Lake Michigan",
  LH = "Lake Huron",
  LS = "Lake St. Clair",
  AN = "Atlantic North",
  AM = "Atlantic South",
  GM = "Gulf of Mexico",
  PH = "Hawaii Waters",
  PZ = "Pacific East"
  )

state.abb <- c(state.abb, names(places))
state.name <- c(state.name, places)
states <- cbind.data.frame(state.abb, state.name)

setdiff(states$state.abb, storm_data$STATE)

```

All state (or places, should we say) abbreviations in our dataset now have a matching pair.

#### Solving the too many event types issue

There are 985 different values in the `EVTYPE` variable; there should only be 48. We have to, somehow, categorize the remaining 937 unique values as one of the official 48 weather event types. 

A lot of the surplus in event types is due to typos (human errors). Also, there are inconsistencies in data entry, which can be noticed by reading the [data codebook](./docs/storm_data_preparation.pdf) provided by the NOAA; e.g. coding an event as "Microdust", while, in the manual, is categorized as "Thunderstorm Winds". 

We can solve many of these inconsistencies by matching them with the closest official `EVTYPE` string. We do this using the longest common substring method:

"The longest common substring (method='lcs') is defined as the longest string that can be obtained by pairing characters from a and b while keeping the order of characters intact. The lcs-distance is defined as the number of unpaired characters. The distance is equivalent to the edit distance allowing only deletions and insertions, each with weight one."

* van der Loo M (2014). “The stringdist package for approximate string matching.” _The R Journal_, *6*, pp. 111-122. <URL: https://CRAN.R-project.org/package=stringdist>.

In this case, given an official event type, we pair it with the closest element of `EVTYPE`. The drawback of this approach is that we got many "false positives" -- like "fog" being matched to "flood", instead of "dense fog".

Given that there are many values in `EVTYPE` that are semantically close to the official event types, but not string-distance close, and the false positive matches, there was still a lot of manual work to do. 

The series of code chunks below has all the steps to arrive at the clean `EVTYPE` variable. 

We need to define the target `EVTYPE` values, which are the 48 official event types defined by the NOAA: 

```{r evtype cleaning}

official_events <- c(
  "Astronomical Low Tide",
  "Astronomical High Tide",
  "Avalanche",
  "Blizzard",
  "Coastal Flood",
  "Cold/Wind Chill",
  "Dense Fog",
  "Dense Smoke",
  "Drought",
  "Dust Devil",
  "Dust Storm",
  "Excessive Heat",
  "Extreme Cold/Wind Chill",
  "Flash Flood",
  "Flood",
  "Frost/Freeze",
  "Funnel Cloud",
  "Freezing Fog",
  "Hail",
  "Heat",
  "Heavy Rain",
  "Heavy Snow",
  "High Surf",
  "High Wind",
  "Hurricane (Typhoon)",
  "Ice Storm",
  "Lake-Effect Snow",
  "Lakeshore Flood",
  "Lightning",
  "Marine Hail",
  "Marine High Wind",
  "Marine Strong Wind",
  "Marine Thunderstorm Wind",
  "Other",
  "Rip Current",
  "Seiche",
  "Sleet",
  "Storm Surge/Tide",
  "Strong Wind",
  "Thunderstorm Wind",
  "Tornado",
  "Tropical Depression",
  "Tropical Storm",
  "Tsunami",
  "Volcanic Ash",
  "Waterspout",
  "Wildfire",
  "Winter Storm",
  "Winter Weather"
)

```

We do some general manual susbstitutions to make all strings a bit more similar: everything lower case, delete non-alphanumeric characters, and "and" words.  

```{r general edits}

storm_data <- storm_data %>% mutate(
  EVTYPE = tolower(EVTYPE),
  EVTYPE = gsub(
    pattern = "[^[:alpha:]]+",
    replacement = " ",
    x = EVTYPE
  ),
  EVTYPE = gsub(
    pattern = "and",
    replacement = " ",
    x = EVTYPE,
    fixed = TRUE
  )
)

```

After reading the [data codebook](./docs/storm_data_preparation.pdf), many semantical mismatches were evident: 

```{r manual-substitutions, cache=TRUE}

## Semantic substitutions, from the codebook
### tstm --> thunderstorm: very common, not close for string matching
storm_data$EVTYPE <- gsub(pattern = "tstm",
                          replacement = "thunderstorm",
                          x = storm_data$EVTYPE)

### Floods homologation of "flood" variants, including urban and rural floods
storm_data$EVTYPE[grepl(pattern = "fld|urban\\/sml stream fld|urban\\/small stream flooding|stream flood", x = storm_data$EVTYPE)]  <-
  "Flood"

### Homologation of "wind" to strong wind
storm_data$EVTYPE <- gsub(pattern = "winds|wnd",
                          replacement = "strong wind",
                          x = storm_data$EVTYPE)

## Homologation of frost/freeze variants
storm_data$EVTYPE[grepl(pattern = "frost|freeze|freezing|frost|icy roads",
                        x = storm_data$EVTYPE,
                        ignore.case = TRUE)] <- "Frost/Freeze"

### cold weather --> winter weather
storm_data$EVTYPE[grepl(pattern = "cold weather",
                        x = storm_data$EVTYPE,
                        ignore.case = TRUE)] <- "Winter Weather"

## Homologation of extreme cold wind chill
storm_data$EVTYPE[grepl(pattern = "cold|chill|record low|cool|low temperature|Hypothermia",
                        x = storm_data$EVTYPE,
                        ignore.case = TRUE)] <-
  "Extreme Cold/Wind Chill"

## Homologation of hurricane (typhoon)
storm_data$EVTYPE[grepl(pattern = "hurricane|typhoon",
                        x = storm_data$EVTYPE,
                        ignore.case = TRUE)] <-
  "Hurricane (Typhoon)"

## Semantic homologation of light snow --> sleet
storm_data$EVTYPE[grepl(pattern = "ligth snow",
                        x = storm_data$EVTYPE,
                        ignore.case = TRUE)] <- "sleet"

## Snow occurrences to heavy snow
storm_data$EVTYPE[grepl(pattern = "snow",
                        x = storm_data$EVTYPE,
                        ignore.case = TRUE)] <- "Heavy Snow"

## Semantic homologation of ~burst to thunderstorm winds
storm_data$EVTYPE[grepl(pattern = "dry microburst|downburst|burst",
                        x = storm_data$EVTYPE,
                        ignore.case = TRUE)] <-
  "thunderstorm wind"

## Semantic homologation of hail
storm_data$EVTYPE[grepl(pattern = "wintry mix|glaze|hail",
                        x = storm_data$EVTYPE,
                        ignore.case = TRUE)] <- "hail"

## Drought semantic homologation
storm_data$EVTYPE[grepl(pattern = "dry|low rainfall",
                        x = storm_data$EVTYPE,
                        ignore.case = TRUE)] <- "Drought"

## Excessive heat
storm_data$EVTYPE[grepl(pattern = "record heat|warm|hot|warmth|high temperature record",
                        x = storm_data$EVTYPE,
                        ignore.case = TRUE)] <- "Excessive Heat"

## After cleaning drought, substituting synonyms of heavy rain
storm_data$EVTYPE[grepl(pattern = "precipitation|rainfall|rain",
                        x = storm_data$EVTYPE,
                        ignore.case = TRUE)] <- "Heavy Rain"

## all surf occurrences to high surf
storm_data$EVTYPE[grepl(pattern = "surf",
                        x = storm_data$EVTYPE,
                        ignore.case = TRUE)] <- "High Surf"

# Ice storm
storm_data$EVTYPE[grepl(pattern = "black ice|ice",
                        x = storm_data$EVTYPE,
                        ignore.case = TRUE)] <- "ice storm"

## blowing dust to dust storm
storm_data$EVTYPE[grepl(pattern = "blowing dust",
                        x = storm_data$EVTYPE,
                        ignore.case = TRUE)] <- "dust storm"

## coastal storm to marine thunderstorm
storm_data$EVTYPE[grepl(pattern = "coastal storm|beach",
                        x = storm_data$EVTYPE,
                        ignore.case = TRUE)] <-
  "marine thunderstorm"

## all funnels to funnel cloud
storm_data$EVTYPE[grepl(pattern = "funnel",
                        x = storm_data$EVTYPE,
                        ignore.case = TRUE)] <- "funnel cloud"

## flood consequences to flash flood (from codebook)
storm_data$EVTYPE[grepl(pattern = "river|slide|dam|ice floes",
                        x = storm_data$EVTYPE,
                        ignore.case = TRUE)] <- "Flash Flood"

## Manual matching of fog to dense fog
storm_data$EVTYPE[grepl(pattern = "fog",
                        x = storm_data$EVTYPE,
                        ignore.case = TRUE)] <- "Dense Fog"

## there were some ambiguous records, that had to be matched to "Other"
others <- c(
  "apache county",
  "none",
  "summary",
  "unseasonably wet",
  "monthly temperature",
  "record temperature",
  "red flag",
  "temperature record"
)

storm_data$EVTYPE[grepl(
  pattern = paste(others, collapse = "|"),
  x = storm_data$EVTYPE,
  ignore.case = TRUE
)] <- "Other"

```

We have a slightly more homogeneous `EVTYPE` variable that is close enough to the official name events. We use the longest common substring method to find how close are each of the values in `EVTYPE` to each of the `official_events`. 

```{r longest-common-substring, cache=TRUE}

## Distance matrix of official_events by EVTYPE
lcs_dist <- sapply(
  X = tolower(official_events),
  FUN = function(x) {
    stringdist(tolower(storm_data$EVTYPE), x, method = "lcs")
  }
)
## Selecting the minimum distance from official_events to EVTYPE
## ie, the official_event type match to the raw EVTYPE
min_lcs <- apply(X = lcs_dist, MARGIN = 1, FUN = which.min)

## Substitutting EVTYPE with the matching official_events
storm_data$EVTYPE <- official_events[min_lcs]

```


#### Estimating the human casualties

Sadly, these weather events cause human deaths and injuries; these numbers are recorded by the NOAA, and will be added to fulfill the definition of "harmful to human health" specified in this document. 

```{r}
storm_data$human_damage <- storm_data$INJURIES + storm_data$FATALITIES
```




#### Calculating the and economic damage to crops and properties

The numbers of human causalties and economic damages are not expressed in their final form; they're rather expressed as coefficients of 10 raised to some power of ten.

`PROPDMG` needs to be multiplied by `PROPDMGEXP` elevated to some power of ten; the same for `CROPDMG * (CROPDMGEXP ^ n)`. 

`PROPDMGEXP` and `CROPDMGEXP` are not explicitly defined as powers of then but rather as some sort abbreviated metrix prefixes: "k" (kilo), "m" (mega or millions), "b" (billions). 

```{r, message=FALSE}
## we have unequal lengths of EXP variables, we unify them by transforming them to lowercase
storm_data$PROPDMGEXP <- tolower(storm_data$PROPDMGEXP)
storm_data$CROPDMGEXP <- tolower(storm_data$CROPDMGEXP)

## defining their unique values 
propdmg_exp_raw <- storm_data$PROPDMGEXP %>% unique
cropdmg_exp_raw <- storm_data$CROPDMGEXP %>% unique

## Merge both variables, get the unique values and sort them alphabetically
dmg_exp_raw <-
  c(propdmg_exp_raw, cropdmg_exp_raw) %>% unique %>% sort

## Deleting non-alphanumeric characters; +.,-,? are ommited
dmg_exp_raw <-
  dmg_exp_raw[grepl(pattern = "[[:alnum:]]", x = dmg_exp_raw)]

## Abbreviation for each power, explained
b <- 9
h <- 2
k <- 3
m <- 6

## Scientific notation
dmg_exp_str <- paste("10^", dmg_exp_raw, sep = "")
dmg_exp <- sapply(parse(text = dmg_exp_str), eval)

## tables with computable powers of ten to join to the storm dataset
prop_dmg_exp_table <-
  dplyr::data_frame(PROPDMGEXP = dmg_exp_raw, prop_dmg_exp = dmg_exp)
crop_dmg_exp_table <-
  dplyr::data_frame(CROPDMGEXP = dmg_exp_raw, crop_dmg_exp = dmg_exp)

storm_data <- storm_data %>%
  right_join(x = prop_dmg_exp_table) %>%
  right_join(x = crop_dmg_exp_table)

```




## Results

### Most harmful meteorological events

Across the United States, which types of events (as indicated in the `EVTYPE` variable) are most harmful with respect to population health?

```{r harmful events}

```


### Most costly meteorological events

Across the United States, which types of events have the greatest economic consequences?

```{r costly events}

```


## References


